+++
title = "Least Squares Estimation"

# date = {{ .Date }}
lastmod = 2020-07-14

draft = false  # Is this a draft? true/false
toc = true  # Show table of contents? true/false
type = "docs"  # Do not modify.

# Responsible teacher (email)
responsibleteacher = "StatisticalSignalProcessing@groups.tue.nl"

# Add menu entry to sidebar.
[menu.5cta0]
name = "2.1 Least Squares Estimation"
weight = 210


+++
## Introduction
The application of the least-squares estimation dates back until <i>Gauss</i>, who used it to study planetary motions. The popularity of the least-squares estimation has not decreased since then and it remains a widely applied estimator. The least-squares estimator does not make any probabilistic assumptions on the observed data. Thus, it is suitable in situations where the precise statics is not known. However, due to the lack of statics of the observed data, no assertion about optimality of the estimator can be made.

The concept of the least-squares estimation can be described as follows. A signal is generated by a deterministic signal model $s[n;\theta]$ that is parametrized by a parameter $\theta$. Due to noise or model inaccuracies, the observed data $x[n]$ deviates from the signal model. As the name suggest, the least-squares approach aims to minimize the squared difference between the observed data and the signal model. Thus, the least-squares error criterion is defined as
\begin{equation}
	J(\theta) = \sum_{n=0}^{N-1}(x[n]-s[n;\theta])^2.
  \label{eq:ls_criterion}
\end{equation}

The least-squares estimate $\hat{\theta}$ is the value of the parameter $\theta$ that minimizes \eqref{eq:ls_criterion}, i.e.,
\begin{equation}
	\hat{\theta} = \underset{\theta}{\operatorname{argmin}}J(\theta).
\end{equation}
From this perspective, the least-squares estimator can be considered as an optimization problem of fitting a model to some available data.


---

<b>Example:</b>

Suppose we want to estimate the energy consumption per km of an electric car. For this purpose, we record at each recharge of the car the amount of energy charged $E$ and the distance travelled $D$. Visualizing the recordings give following picture.
<div style="max-width: 900px; margin: auto">
  <figure>
    <img
      src="/../files/7.Images/statistical/estimation/LS_electric_car_1.jpg"
      alt="Recorded energy consumption vs. distance travelled."
    />
    <figcaption class="numbered">
	      Recorded energy consumption vs. distance travelled.
    </figcaption>
  </figure>
</div>

From the figure we can see that the energy consumption increases almost linearly with the distance travelled. Thus, we can assume a linear relation between the energy consumption and the distance travelled:
\begin{equation}
 s[n;\theta] = \theta D[n].
 \label{eq:model}
\end{equation}
In this model, the parameter $\theta$ represents the slope of the line. From the figure, however, we can also see that the data points do not exactly lie on a line. The discrepancy between the model and data comes from the model inaccuracy: the energy consumption depends on more parameters than just the distance. Nonetheless, the suggest model serves our purpose. The least-square error  criterion becomes
\begin{equation}
 J(\theta) = \sum_{n=0}^{N-1} (x[n]-\theta D[n])^2.
\end{equation}
Taking the derivative with respect to $\theta$ and equating it to zero yields
\begin{equation}
		\hat{\theta} = \frac{\sum_{n=0}^{N-1}x[n]D[n]}{\sum_{n=0}^{N-1}D^2[n]}.
		\label{eq:electric_car}
\end{equation}

<div style="max-width: 900px; margin: auto">
  <figure>
    <img
      src="/../files/7.Images/statistical/estimation/LS_electric_car_2.jpg"
      alt="Line fitted to the data."
    />
    <figcaption class="numbered">
	      Line fitted to the data.
    </figcaption>
  </figure>
</div>

<br></br>

<div class="video-container">
<iframe width="100%" height="100%" src="https://www.youtube.com/embed/F2_a5u8bX3w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<br></br>


---
In the example above, we were only interested in estimating a single parameter $\theta$. In general, the signal model can be dependent on several parameters. Consider for example, the signal model  
\begin{equation}
	s[n; A, f_0] = A\cos(2\pi f_0 n)
\end{equation}
where $A$ and $f_0$ are the amplitude and the frequency of the signal, respectively. This signal model is parametrized by two parameters $A$ and $f_0$, which can be expressed in a vector $\boldsymbol\theta = [A, f_0]^T$. In this case we write $s[n;\boldsymbol\theta]$. In the following, we will consider vector parameters since the scalar case arise as a special case of the vector parameters.

Depending on the signal model, we can distinguish between linear and nonlinear least-squares estimators. The electric car example was a linear estimation problem, whereas the signal model
\begin{equation}
s[n,f_0] = A \cos(2\pi f_0 n)
\end{equation}
is nonlinear in the parameter $f_0$. The focus of this module is the linear least squares estimator. Nonlinear least squares estimators rely on numerical methods which are treated separately in the module <a href="../statisticalsignalprocessing_estimation_numerical">Numerical Methods</a>. However, for some particular nonlinear problems it is possible tor convert them either into a linear problem or to separate them into a linear and a nonlinear part. These two special cases are covered in this module.

## Linear Least Squares Estimator
A linear signal model is any model of the form
\begin{equation}
  s[n;\boldsymbol\theta] = \sum_{k=0}^{K-1} h_k[n]\theta_k.
\end{equation}
Typically, more than one observation is available. In this case, the linear least-squares estimator can be expressed as
\begin{equation}
  \mathbf{s} = \mathbf{H}\boldsymbol\theta
  \label{eq:ls_linear}
\end{equation}
where $\mathbf{s}$ is the signal vector, $\mathbf{H}$ is the so called <i>observation matrix</i>, and $\boldsymbol\theta$ is the parameter vector.


---
<b>Example:</b>

Assume our signal model is a polynomial of degree $K-1$ with coefficients $\theta_0,\theta_1,\dots,\theta_{K-1}$, i.e.,
\begin{equation}
  s[n;\theta] = \theta_{0}n^{0} + \theta_{1}n^{1}+ \dots + \theta_{K-1}n^{K-1},
\end{equation}
and suppose observations for $n=0,1,\dots,N-1$ are given. The corresponding observation matrix $\mathbf{H}$ is
\begin{equation}
  \mathbf{H} =
  \begin{bmatrix}
      1  & 0 & 0 &\cdots &0\newline
      1  & 1 & 1& & 1\newline
      1  & 2 & 2^2 & & 2^{K-1} \newline
      \vdots & & & \ddots\newline
      1 & N-1 & (N-1)^2 & \cdots & (N-1)^{K-1}
  \end{bmatrix}.
\end{equation}

---

Using the matrix notation in \eqref{eq:ls_linear}, the least-squares error criterion in \eqref{eq:ls_criterion} can be expressed as
\begin{align}
  J(\boldsymbol\theta)&= ||\mathbf{x}-\mathbf{H}\boldsymbol\theta||^2\nonumber\newline
											&=(\mathbf{x}-\mathbf{H}\boldsymbol\theta)^T(\mathbf{x}-\mathbf{H}\boldsymbol\theta)\nonumber\newline
                      &= (\mathbf{x}^T-\boldsymbol\theta^T\mathbf{H^T})(\mathbf{x}-\mathbf{H}\boldsymbol\theta) \nonumber \newline
                      &= \mathbf{x}\mathbf{x}^T - 2 \mathbf{x}^T \mathbf{H}\boldsymbol\theta +\boldsymbol\theta^T\mathbf{H}^T \mathbf{H}\boldsymbol\theta.
\end{align}
The corresponding gradient is
\begin{equation}
    \frac{\partial J(\boldsymbol\theta)}{\partial \boldsymbol\theta}  = -2\mathbf{H}^T\mathbf{x}+2\mathbf{H}^T\mathbf{H}\boldsymbol\theta.
\end{equation}
The minimum can be found by setting the gradient to zeros and solving for $\boldsymbol\theta$. The linear least-squares estimator is then
\begin{equation}
    \hat{\boldsymbol\theta} = (\mathbf{H}^T\mathbf{H})^{-1}  \mathbf{H}^T \mathbf{x},
\end{equation}
which is referred to as <i>normal equation</i>. Returning to the electric car example, we have the special case of a single parameter $\theta$. In this case, the observation matrix becomes a vector with the distances $D[n]$ as its entries. The expression $\mathbf{H}^T\mathbf{H} = \sum_{n=0}^{N-1}D^2[n]$. Note that $\sum_{n=0}^{N-1}D^2[n]$ is a scalar and its inverse is $1/(\sum_{n=0}^{N-1}D^2[n])$. On the other hand, the expression $\mathbf{H}^T\mathbf{x}$ is $\sum_{n=0}^{N-1}x[n]D[n]$. Combining these two expressions we obtain the least-squares estimate in \eqref{eq:electric_car}.

### Geometric Interpretation
The previous derivation is based on minimizing the squared error term in \eqref{eq:ls_criterion}. The least-square estimation can also be derived based on geometrical interpretations. To obtain this geometrical interpretation, we assume that more observations than parameters to be estimated are available, i.e., $N>K$. With this assumption, we can interpret \eqref{eq:ls_linear} from a geometric point of view. The vector $\mathbf{s}$ and the column space of $\mathbf{H}$ have dimension $N$. However, since $K<M$, the linear combination
\begin{equation}
\mathbf{s} = 	\mathbf{H}\boldsymbol\theta = \sum_{k=0}^{K-1} \theta_k \mathbf{h}_k,
\end{equation}
where $\mathbf{h}_k$ is the $k$th column of $\mathbf{H}$, spans only a $K$ dimensional subspace of the $N$-dimensional space. This is examplified in the next figure for the case $N=3$ and $K=2$.

<div style="max-width: 1200px; margin: auto">
  <figure>
    <img
      src="/../files/7.Images/statistical/estimation/span.svg"
      alt="The data vector $\mathbf{x}$ does not lie in the span of $\mathbf{H}$."
    />
    <figcaption class="numbered">
	      The data vector $\mathbf{x}$ does not lie in the subspace spanned by the columns of $\mathbf{H}$, which is indicated by the red plane.
    </figcaption>
  </figure>
</div>

From the geometric point of view, the least-squares error criterion in \eqref{eq:ls_linear} describes the length of the error vector, i.e., the vector between data vector $\mathbf{x}$ and the $\mathbf{s}$ which is in the subspace. The length of the error vector becomes a minimum if it is orthogonal to the subspace, which is known as the <i>orthogonality principle</i>. An example of an orthogonal error vector is illustrated below.

<div style="max-width: 1200px; margin: auto">
  <figure>
    <img
      src="/../files/7.Images/statistical/estimation/projection.svg"
      alt="The length of the error vector becomes a minimum if it is orthogonal to the subspace spanned by the columns of $\mathbf{H}$."
    />
    <figcaption class="numbered">
	      The length of the error vector becomes a minimum if it is orthogonal to the subspace spanned by the columns of $\mathbf{H}$.
    </figcaption>
  </figure>
</div>

The error vector is orthogonal to the subspace if it is orthogonal to all vectors which span the space, i.e.,
\begin{align}
\mathbf{H}^T \mathbf{e} = \mathbf{H}^T (\mathbf{x-H}\boldsymbol\theta) = \mathbf{0}
\end{align}
with $\mathbf{0}$ denoting the zero vector.

\begin{equation}
\hat{\boldsymbol\theta} = (\mathbf{H}^T\mathbf{H})^{-1}\mathbf{H}^T\mathbf{x}
\end{equation}

### Weighted Least Squares Estimation

For some estimation problems, we might want to reduce the influence of a portion of the data on our final estimate. For example, the data of the can be provided by different sensors. Moreover, some sensors may have a higher accuracy than others, and thus, we have more confidence in their measurement result. However, even though the other sensor measurement results are less accurate, they still provide some information. The different confidence in the measurement result can be incooperated in least-squares estimator by assigning them different weights, which leads to the weighted least-squares estimator. In general, we can express the weighted least-squares error criterion as
\begin{equation}
J(\boldsymbol\theta) = (\mathbf{x}-\mathbf{H}\boldsymbol\theta)^T\mathbf{W}(\mathbf{x}-\mathbf{H}\boldsymbol\theta)
\end{equation}
where $\mathbf{W}$ is a positive definite matrix. For the particular case that $\mathbf{W}$ is diagonal, we obtain
\begin{equation}
J(\boldsymbol\theta) = \sum_{n=0}^{N-1} w_n (x[n]-s[n;\boldsymbol\theta])^2,
\end{equation}
where $w_n$ is the $n$th diagonal element of $\mathbf{W}$.

The weighted least squares estimator is obtained by setting the gradient to zero, which yields
\begin{equation}
\hat{\boldsymbol\theta}=(\mathbf{H}^T\mathbf{W}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{W}\mathbf{x}.
\end{equation}
**add a reference to other estimators**


## Nonlinear Least Squares Estimator

Until now, we dealt with linear signal models, i.e., functions that are linear in the parameter vector $\boldsymbol\theta$. For this particular case, we were able to derive a closed form expression for the least-squares estimate based on the observation matrix $\mathbf{H}$. In general, the signal model can also be nonlinear in the parameter vector $\boldsymbol\theta$. Thus, it cannot be expressed as in \eqref{eq:ls_linear}. Instead, we express the least-squares error criterion as
\begin{equation}
		J(\boldsymbol\theta) = (\mathbf{x}-\mathbf{s}(\boldsymbol\theta))^T(\mathbf{x}-\mathbf{s}(\boldsymbol\theta))
\end{equation}
Minimizing the cost function $J(\boldsymbol\theta)$ is more difficult and typically relies on numerical optimization methods. However, for some problems it is possible to transform them into linear problems or separate them in a linear and a nonlinear part. Separating the problem allows to reduce the complexity of the optimization since for the linear part, close forms solution are available. In the following, we examine these specific cases.

### Transformation of Parameters

Optimization problems have the property that they can be carried out in a transformed space that is obtained by a one-to-one mapping. Once the minimum is found in the transformed space, it can be transformed back to the original space. This property can be used to produce a linear signal model. Therefore, let
\begin{equation}
\boldsymbol\alpha = g(\boldsymbol\theta)
\end{equation}
be a functions whose inverse exists. If we can find a function $g(\boldsymbol\theta)$ for which the 


The first method of dealing with nonlinear problems can be described as finding our way back to the linear LS problem by applying an invertible transformation to the parameters to be estimated. If we can find a transformation
\begin{equation}
\alpha=f(\theta)
\end{equation}
such that
\begin{equation}
\mathbf{s}(\theta)=\mathbf{s}(f^{-1}(\alpha))=\mathbf{H}\alpha
\end{equation}
then we can simply use the linear LS formulations we derived so far and find the parameter $\theta$ through the inverse transform.

<b>Example:</b>

---
The relation between the phase of a sinusoidal signal and the data samples from that signal is nonlinear:
\begin{equation}
s[n,\theta] = \sin(2\pi fn + \theta).
\end{equation}
It is possible to transform the signal such that the signal model is in a linear relation with a function of the phase term $\theta$. Consider the trigonometric identity:
\begin{equation}
\sin(A+B)=\sin(A)\cos(B)+\cos(A)\sin(B).
\end{equation}
Then the signal model can be rewritten as
\begin{equation}
s[n,\theta]=\sin(2\pi fn)\cos(\theta) + cos(2\pi fn)\sin(\theta).
\end{equation}
To implement least squares estimation, we substitute the phase $\theta$ with two other parameters, $\alpha_1=\cos(\theta)$ and $\alpha_2=\sin(\theta)$. The signal model in vector form becomes
\begin{equation}
\mathbf{s}(\theta) = \mathbf{s}(f^{-1}(\alpha))=\mathbf{H}\alpha
\end{equation}
where $\mathbf{H}=[\mathbf{S}\mathbf{C}]$ is the observation matrix consisting of two columns,
\begin{equation}
\mathbf{S}=\big[0 ~ \sin(2\pi f) ~ \sin(2\pi 2f) ~ ... ~ \sin(2\pi Nf)\big]^T,
\end{equation}
\begin{equation}
\mathbf{C}=\big[1 ~ \cos(2\pi f) ~ \cos(2\pi 2f) ~ ... ~ \cos(2\pi Nf)\big]^T,
\end{equation}
and the parameter vector is $\alpha=[\alpha_1 ~~ \alpha_2]^T$

---



<!---
## Least Squares Estimator for Vector Parameters

Estimation problems in general deal with models that are controlled by multiple parameters. Consider a line fitting example where the model is
\begin{equation}
s[n,\Theta]=An+B,
\end{equation}
where the east squares estimation for both parameters $A$ and $B$ have to be found. In other words, $\Theta=[A B]^T$. This means there are two minimization problems to be solved:
\begin{equation}
\frac{\partial}{\partial A}\sum_{n=0}^{N-1}(x[n]-An-B)^2=0
\end{equation}
\begin{equation}
\frac{\partial}{\partial B}\sum_{n=0}^{N-1}(x[n]-An-B)^2=0
\end{equation}
Both minimization problems can be combined into a single expression that can be solved using linear algebra methods. The first step is to write the model in linear algebraic form:
\begin{equation}
s[n;\Theta]=\mathbf{H\Theta}
\end{equation}
where $\mathbf{H}$ is called <b>the observation matrix</b>.

The cost function that is the sum of squared error terms becomes
\begin{equation}
J(\Theta)=(\mathbf{x}-\mathbf{H}\Theta)^T(\mathbf{x}-\mathbf{H}\Theta)
\end{equation}

The derivative operation in linear algebraic form is
\begin{equation}
\frac{\partial}{\partial\Theta}[(\mathbf{x}-\mathbf{H}\Theta)^T(\mathbf{x}-\mathbf{H}\Theta)]=-2\mathbf{H}^T\mathbf{x}+2\mathbf{H}^T\mathbf{H}\Theta=0
\end{equation}
The least squares estimation formulation is obtained as
\begin{equation}
\hat\Theta_{LS}=(\mathbf{H}^T\mathbf{H})^{-1}\mathbf{H}^T\mathbf{x}.
\end{equation}

### Weighted Least Squares Estimation

For some estimation problems, we might want to reduce the influence of a portion of the data on our final estimate. For example, consider again the problem of setting up a budget for the energy expenditure of an electric car. It is possible that the energy consumption per kilometer for driving in the city is different from driving a long distance on a highway. So, we may want to derive two separate estimations for the energy consumption to reflect on the difference. Weighted least squares estimation serves this purpose by introducing a weight $W_n$ to the squared error term:
\begin{equation}
J(\Theta) = \sum_{n=0}^{N-1}W_n(x[n]-s[n;\Theta])^2.
\end{equation}
The squared error term expression can be written in linear algebraic form:
\begin{equation}
J(\Theta)=(\mathbf{x}-\mathbf{H}\Theta)^TW(\mathbf{x}-\mathbf{H}\Theta)
\end{equation}
where $W$ is a diagonal matrix such that $W_{nn}=W_n$ is the entry on the $n^{th}$ row and column.

The weighted least squares estimator is obtained by setting the derivative of the squared error term to zero, which yields
\begin{equation}
\hat\Theta_{LS}=(\mathbf{H}^TW\mathbf{H})^{-1}\mathbf{H}^TW\mathbf{x}.
\end{equation}

## Nonlinear Least Squares Estimation

Until now, we focused our attention on signal models that are a linear function of the controlling parameter $\theta$. We were able to derive a closed form expression for the LSE that operates on the data $\mathbf{x}$ with the observation mtrix $\mathbf{H}$. Not all problems can be cast into a linear LSE. In this section, we will investigate several ways of dealing with nonlinear LSE. Numerical solution methods are covered at the end of the Estimation Theory part of the course.

### Transforming a Nonlinear Problem to a Linear Problem

The first method of dealing with nonlinear problems can be described as finding our way back to the linear LS problem by applying an invertible transformation to the parameters to be estimated. If we can find a transformation
\begin{equation}
\alpha=f(\theta)
\end{equation}
such that
\begin{equation}
\mathbf{s}(\theta)=\mathbf{s}(f^{-1}(\alpha))=\mathbf{H}\alpha
\end{equation}
then we can simply use the linear LS formulations we derived so far and find the parameter $\theta$ through the inverse transform.

<b>Example:</b>

---
The relation between the phase of a sinusoidal signal and the data samples from that signal is nonlinear:
\begin{equation}
s[n,\theta] = \sin(2\pi fn + \theta).
\end{equation}
It is possible to transform the signal such that the signal model is in a linear relation with a function of the phase term $\theta$. Consider the trigonometric identity:
\begin{equation}
\sin(A+B)=\sin(A)\cos(B)+\cos(A)\sin(B).
\end{equation}
Then the signal model can be rewritten as
\begin{equation}
s[n,\theta]=\sin(2\pi fn)\cos(\theta) + cos(2\pi fn)\sin(\theta).
\end{equation}
To implement least squares estimation, we substitute the phase $\theta$ with two other parameters, $\alpha_1=\cos(\theta)$ and $\alpha_2=\sin(\theta)$. The signal model in vector form becomes
\begin{equation}
\mathbf{s}(\theta) = \mathbf{s}(f^{-1}(\alpha))=\mathbf{H}\alpha
\end{equation}
where $\mathbf{H}=[\mathbf{S}\mathbf{C}]$ is the observation matrix consisting of two columns,
\begin{equation}
\mathbf{S}=\big[0 ~ \sin(2\pi f) ~ \sin(2\pi 2f) ~ ... ~ \sin(2\pi Nf)\big]^T,
\end{equation}
\begin{equation}
\mathbf{C}=\big[1 ~ \cos(2\pi f) ~ \cos(2\pi 2f) ~ ... ~ \cos(2\pi Nf)\big]^T,
\end{equation}
and the parameter vector is $\alpha=[\alpha_1 ~~ \alpha_2]^T$

---

### Separating the Nonlinear and Linear Parts of a Problem

The second method of dealing with nonlinear problems consist of dividing the problem into linear and non-linear parts. The parameters to be estimated are dividen into two sets such that $\Theta=[\Theta_a~\Theta_b]^T$, where
\begin{equation}
\mathbf{s}(\Theta)=\mathbf{H}(\Theta_a)\Theta_b.
\end{equation}
In other words, we write the signal model such that some of the parameters to be estimated, $\Theta_a$, are left inside the observation matrix, which is in a linear relation with the remaining parameters $\Theta_b$. We can still use the linear LSE for estimating the parameters $\Theta_b$, after finding the parameters $\Theta_a$.


<b>Example:</b>

---
Consider a capacitor-resistor circuit where the capacitor is charged up to a voltage level $V_b$ and then left to discharge over the resistor. The measured voltage between the capacitor's contact points is given by
\begin{equation}
x[n]=V_b e^{-\frac{nT_s}{RC}},
\end{equation}
where $T_s$ is the sampling period, $R$ is the resistance value and $C$ is the capacitance value. To estimate the value of $RC$ and $V_b$ from a set of $N$ samples, we separate the problem into non-linear and linear parts:
\begin{equation}
\mathbf{x}=\mathbf{H}(RC)V_b,
\end{equation}  
where $\mathbf{H}(RC)=[h~ h^2~ ...~ h^N]^T$ is the observation matrix and
\begin{equation}
h=e^{-\frac{T_s}{RC}}.
\end{equation}
The solution to $V_b$ can be found by
\begin{equation}
\hat{V}_b=\big(\mathbf{H}^T(RC)\mathbf{H}(RC)\big)^{-1}\mathbf{H}^T(RC)\mathbf{x},
\end{equation}
if the value for RC can be found by another method.

---

### The Grid Search

The closed form solution to the LSE gives the parameter value that minimizes the squared error function. Another method to locate the minimum of the squared error function is to sample that function on possible values of the parameter $\theta$ and pick the one that leads to the smallest squared error value. For a parameter vector $\Theta$, each dimension has to be sampled in search for the minimum of the error function. We can imagine the sampling points constituting a grid in the two dimensional case, hence, the name <i>grid search</i>.

The grid search is computationally expensive, especially when the parameter vector $\Theta$ has more than a few dimensions. Eventually, the grid search corresponds to evaluating the squared error for each sample in the parameter space. Thus, we have to limit the number of samples by limiting the search range and keeping the sample granularity coarse.


## Conclusion

The least squares estimation is an intuitive estimation technique that measures the goodness of fit between the data and the signal model through the squared error term. It is applicable to a variety of problems and finds frequent use in linear regression. A caveat regarding the LSE is the possibility of over-fitting. It is possible to come up with a signal model with enough number of parameters to reduce the squared error to zero for a data set. Failure of such models become evident only when the squared error is calculated for a new data set.

There is no criteria that tells how much residual error has to remain after a model is fitted. Such criteria requires more information on the noise, which is not considered in the LSE. We will see in the remaining modules the paradigm shift required to take the noise properties into consideration when designing estimators. LSE can be augmented with information on noise properties; we will revisit the LSE later to see an example of such augmentation.
--->
